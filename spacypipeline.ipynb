{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eb8e85-23a7-4d21-b1e7-1de9a25b55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8afa04-0ce2-45c1-87fd-6d408387704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a046be5a-3c39-4e23-a7b5-24f6ebb73726",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3d3b43-d807-4212-be78-3832a10e6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd268014-ecec-49f9-8495-877a3ee5ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9d1c6e-154b-4f93-b4bf-99eff1c24023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f9d9633-cb35-4a4e-a917-7a2696763b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe9b8d4f-337c-42d2-8f35-04f7f23ee546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14626626061804382878"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa005774-4f82-4e4d-9049-853a48f9596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ad15f70-dfc0-4c58-a855-68645ca293b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "977dca33-4781-4e57-9898-0ce3bb8e8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b453f75-9845-4b7c-b338-aa10d3a2c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters | NOUN | character\n",
      "like | ADP | like\n",
      "periods | NOUN | period\n",
      ", | PUNCT | ,\n",
      "exclamation | NOUN | exclamation\n",
      "point | NOUN | point\n",
      "and | CCONJ | and\n",
      "newline | ADJ | newline\n",
      "char | NOUN | char\n",
      "are | AUX | be\n",
      "used | VERB | use\n",
      "to | PART | to\n",
      "separate | VERB | separate\n",
      "the | DET | the\n",
      "sentences | NOUN | sentence\n",
      ". | PUNCT | .\n",
      "But | CCONJ | but\n",
      "one | NUM | one\n",
      "drawback | NOUN | drawback\n",
      "with | ADP | with\n",
      "split | NOUN | split\n",
      "( | PUNCT | (\n",
      ") | PUNCT | )\n",
      "method | NOUN | method\n",
      ", | PUNCT | ,\n",
      "that | SCONJ | that\n",
      "we | PRON | we\n",
      "can | AUX | can\n",
      "only | ADV | only\n",
      "use | VERB | use\n",
      "one | NUM | one\n",
      "separator | NOUN | separator\n",
      "at | ADP | at\n",
      "a | DET | a\n",
      "time | NOUN | time\n",
      "! | PUNCT | !\n",
      "So | ADV | so\n",
      "sentence | NOUN | sentence\n",
      "tonenization | NOUN | tonenization\n",
      "wo | AUX | will\n",
      "nt | PART | not\n",
      "be | AUX | be\n",
      "foolproof | ADJ | foolproof\n",
      "with | ADP | with\n",
      "split | NOUN | split\n",
      "( | PUNCT | (\n",
      ") | PUNCT | )\n",
      "method | NOUN | method\n",
      ". | PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token,\"|\",token.pos_,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90631e8f-8ea8-42c6-ac1b-8792e9058be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL\n",
      "CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ents in doc.ents:\n",
    "    print(token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a389858-7148-42a0-8fb2-0fd68f68cbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95db0f-c22f-4716-aafa-a1f8063312c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
